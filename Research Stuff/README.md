This folder is used for contents related to experimentation and research.  

Such content may include:

- sample programs
- experimentation files
- research notes and ideas

# Table of Contents

- [To-Do List](#todo)
- [Research Material](#research)
- [Possible Datasets](#datasets)

## TO-DO List<a name="todo"></a>

- [ ] Mathematics behind Gradient Descent and Backpropogation (See research material list)
- [X] Word2Vec - The Skip-gram Model
- [X] Find Datasets  
- [ ] Create Final Year Interim Report
- [ ] Code up Word2Vec Implementation

## Research Material<a name="research"></a>

- [YouTube Videos / Playlists](#youtube)
- [Websites](#websites)
- [Books](#books)

### YouTube Videos / Playlists<a name="youtube"></a>

***

- 3Blue1Brown - [Neural Networks](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)

  Goes into the theory and the mathematics behind neural networks in-depth

- The Coding Train - [Neural Networks](https://www.youtube.com/playlist?list=PLRqwX-V7Uu6aCibgK1PTWWu9by6XFdCfh)

  An explanation and implementation of neural networks through a javascript perspective

- The Coding Train - [Intro to Machine Learning - Intelligence and Learning](https://www.youtube.com/playlist?list=PLRqwX-V7Uu6bCN8LKrcMa6zF4FPtXyXYj)
  
  Contains videos on linear regression and the mathematics of gradial descent

### Websites<a name="websites"></a>

***

- [Machine Learning with Python: Neural Networks from Scratch in Python](https://www.python-course.eu/neural_networks.php)
  
  Contains python code for a basic implementation of neural networks

- [Artificial neural network - Wikipedia](https://en.wikipedia.org/wiki/Artificial_neural_network)

- [Mikolov et al Paper on Word2Vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)

- [Vector Representation of Words | TensorFlow](https://www.tensorflow.org/tutorials/representation/word2vec)

  Contains general information on word embeddings

- [Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)

  Looks at **Word2Vec** in more detailS

- [Google's Word2Vec Implementation](https://github.com/dav/word2vec)

  [Click here](https://code.google.com/archive/p/word2vec/) for Google's training data source & implementation explaination

- [Word2Vec | Skymind](https://skymind.ai/wiki/word2vec)

- [Word2Vec Tutorial | RADIM ŘEHŮŘEK](https://rare-technologies.com/word2vec-tutorial/)

  Contains explanation on some of the Gensim API

- [How to Develop Word Embeddings in Python with Gensim](https://machinelearningmastery.com/develop-word-embeddings-python-gensim/)

- [Generating a Word2Vec model from a block of Text using Gensim (Python)](https://codesachin.wordpress.com/2015/10/09/generating-a-word2vec-model-from-a-block-of-text-using-gensim-python/)

  Mentions the Wikipedia API as a source of text

### Books<a name="books"></a>

***

- [Neural Networks and Deep Learning (Online)](http://neuralnetworksanddeeplearning.com/index.html)

  Explanation of neural networks and the mathematics with a view of handwritten text identification
