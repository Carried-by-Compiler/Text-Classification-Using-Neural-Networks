{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import logging\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics\n",
    "import os \n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from tkinter import filedialog\n",
    "from tkinter import *\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "class D2V:\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.__model = Doc2Vec(dm=1,\n",
    "                               vector_size=300,\n",
    "                               min_count=5,\n",
    "                               epochs=50,\n",
    "                               workers=8)\n",
    "\n",
    "    \n",
    "    def train(self, train_corpus):\n",
    "        self.__model.build_vocab(train_corpus)\n",
    "        self.__model.train(train_corpus, total_examples=self.__model.corpus_count, epochs=self.__model.epochs)\n",
    "        return 1\n",
    "\n",
    "    def save(self, folder_path, filename):\n",
    "        self.__model.save(join(folder_path, filename))\n",
    "\n",
    "    def load(self, folder_path, filename):\n",
    "        self.__model = Doc2Vec.load(join(folder_path, filename))\n",
    "\n",
    "    def infer_doc(self, doc):\n",
    "        return self.__model.infer_vector(doc)\n",
    "\n",
    "    def get_vector(self, id):\n",
    "        return self.__model.docvecs[id]\n",
    "\n",
    "    def get_similar(self, doc):\n",
    "        return self.__model.docvecs.most_similar([doc])\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"\n",
    "        Returns the labels of all documents within the Doc2Vec model\n",
    "        \"\"\"\n",
    "        return list(self.__model.docvecs.doctags.keys())\n",
    "\n",
    "    def get_doc_vec(self, identifier: str):\n",
    "        return self.__model.docvecs[identifier]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FileReader:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.__models_paths = \"D:\\\\workspace\\\\Text-Classification-Using-Neural-Networks\\\\src\\\\DocumentClassifier\\\\doc2vec_models\"\n",
    "        self.__training_path = \"D:\\\\workspace\\\\FYP_dataset\\\\training\"\n",
    "        self.__testing_path = \"D:\\\\workspace\\\\FYP_dataset\\\\testing\"\n",
    "\n",
    "    def read_corpus_train(self):\n",
    "\n",
    "        print(\"READING CORPUS\")\n",
    "        topics = dirs = os.listdir(self.__training_path)\n",
    "        \n",
    "        # Go through each foler in dataset; where folder_name = topic of documents in that folder\n",
    "        for topic in dirs:\n",
    "            print(\"Current topic: {}\".format(topic))\n",
    "            curr_path = os.path.join(self.__training_path, topic)\n",
    "            docs = os.listdir(curr_path)\n",
    "\n",
    "            # Create TaggedDocument objects for each document in that folder\n",
    "            for i, document in enumerate(docs):\n",
    "                curr_doc_write = os.path.join(curr_path, document)\n",
    "                with open(curr_doc_write, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "                    content = file.read()\n",
    "\n",
    "                doc_id = topic + \"__\" + str(i) \n",
    "                yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(content), [doc_id])\n",
    "\n",
    "    def get_models_path(self): return self.__models_paths\n",
    "    def get_training_path(self): return self.__training_path\n",
    "    def get_testing_path(self): return self.__testing_path\n",
    "\n",
    "    def process_new_doc(self, filename):\n",
    "        curr_doc_write = os.path.join(self.__testing_path, filename)\n",
    "        with open(curr_doc_write, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "                    content = file.read()\n",
    "        return gensim.utils.simple_preprocess(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.__topics = list()\n",
    "        self.clf = MLPClassifier(activation='logistic', \n",
    "                                 learning_rate=\"adaptive\", \n",
    "                                 learning_rate_init=0.0001,\n",
    "                                 solver=\"adam\",\n",
    "                                 max_iter=500)\n",
    "\n",
    "    def train(self, x: np.array, y: np.array) -> None:\n",
    "        result = self.clf.fit(x, y)\n",
    "        print(\"Training error: {}\".format(result.score(x, y)))\n",
    "\n",
    "    def predict_probability(self, x: np.array) -> np.array:\n",
    "        val = self.clf.predict_proba(x)\n",
    "    \n",
    "        return val\n",
    "\n",
    "    def predict(self, x: np.array) -> np.array:\n",
    "        val = self.clf.predict(x)\n",
    "    \n",
    "        return val\n",
    "    \n",
    "    def get_topics(self): return self.__topics\n",
    "\n",
    "    def add_topic(self, t: str):\n",
    "        if t not in self.__topics:\n",
    "            self.__topics.append(t)\n",
    "\n",
    "    def get_topic_vector(self, t: str):\n",
    "        topic_vec = list()\n",
    "        for topic in self.__topics:\n",
    "            if t == topic:\n",
    "                topic_vec.append(1)\n",
    "            else:\n",
    "                topic_vec.append(0)\n",
    "\n",
    "        return topic_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = FileReader()\n",
    "model = D2V()\n",
    "classifier = NN()\n",
    "\n",
    "model_name = \"Sample_1.d2v\"\n",
    "\n",
    "# All contents of the lists below should be vectorized\n",
    "train_topics = list()\n",
    "train_docs = list() \n",
    "\n",
    "test_topics = list()\n",
    "test_docs = list() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trainset():\n",
    "    \"\"\"\n",
    "    Use this function when a trained Doc2Vec model exists. This function assumes\n",
    "    that a Doc2Vec model is already loaded into the program. The NN classifier\n",
    "    is used to store the necessary topics into the program.\n",
    "    \"\"\"\n",
    "    doc_labels = model.get_labels()\n",
    "    train_topics.clear()\n",
    "    train_docs.clear()\n",
    "    \n",
    "    for label in doc_labels:\n",
    "        train_docs.append(model.get_doc_vec(label))\n",
    "        \n",
    "        split_string = label.split(\"__\")\n",
    "        \n",
    "        train_topics.append(split_string[0])\n",
    "        classifier.add_topic(split_string[0])\n",
    "    \n",
    "    for i in range(len(train_topics)):\n",
    "        train_topics[i] = classifier.get_topic_vector(train_topics[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_testset():\n",
    "    \n",
    "    test_topics.clear()\n",
    "    test_docs.clear()\n",
    "    \n",
    "    print(\"Loading test dataset\")\n",
    "    topics = classifier.get_topics()\n",
    "    \n",
    "    for topic in topics:\n",
    "        print(\"Current topic: %s\" % topic)\n",
    "        file_location = os.path.join(reader.get_testing_path(), topic)\n",
    "        files = os.listdir(file_location)\n",
    "        \n",
    "        for file in files:\n",
    "            with open(os.path.join(file_location, file), mode=\"r\", encoding=\"utf-8\") as file:\n",
    "                    content = file.read()\n",
    "            cleaned_doc = simple_preprocess(content)\n",
    "            \n",
    "            test_topics.append(topic)\n",
    "            test_docs.append(model.infer_doc(cleaned_doc))\n",
    "    \n",
    "    for i in range(len(test_topics)):\n",
    "        test_topics[i] = classifier.get_topic_vector(test_topics[i])\n",
    "    \n",
    "    print(\"Finished loading test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_d2v():\n",
    "    train_corpus = list(reader.read_corpus_train())\n",
    "    \n",
    "    r = model.train(train_corpus)\n",
    "    if r == 1:\n",
    "        model.save(reader.get_models_path(), model_name)\n",
    "        print(\"Doc2Vec training complete & saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-07 14:22:30,844 : INFO : loading Doc2Vec object from D:\\workspace\\Text-Classification-Using-Neural-Networks\\src\\DocumentClassifier\\doc2vec_models\\Sample_1.d2v\n",
      "2019-04-07 14:22:31,207 : INFO : loading vocabulary recursively from D:\\workspace\\Text-Classification-Using-Neural-Networks\\src\\DocumentClassifier\\doc2vec_models\\Sample_1.d2v.vocabulary.* with mmap=None\n",
      "2019-04-07 14:22:31,208 : INFO : loading trainables recursively from D:\\workspace\\Text-Classification-Using-Neural-Networks\\src\\DocumentClassifier\\doc2vec_models\\Sample_1.d2v.trainables.* with mmap=None\n",
      "2019-04-07 14:22:31,209 : INFO : loading syn1neg from D:\\workspace\\Text-Classification-Using-Neural-Networks\\src\\DocumentClassifier\\doc2vec_models\\Sample_1.d2v.trainables.syn1neg.npy with mmap=None\n",
      "2019-04-07 14:22:31,246 : INFO : loading wv recursively from D:\\workspace\\Text-Classification-Using-Neural-Networks\\src\\DocumentClassifier\\doc2vec_models\\Sample_1.d2v.wv.* with mmap=None\n",
      "2019-04-07 14:22:31,247 : INFO : loading vectors from D:\\workspace\\Text-Classification-Using-Neural-Networks\\src\\DocumentClassifier\\doc2vec_models\\Sample_1.d2v.wv.vectors.npy with mmap=None\n",
      "2019-04-07 14:22:31,282 : INFO : loading docvecs recursively from D:\\workspace\\Text-Classification-Using-Neural-Networks\\src\\DocumentClassifier\\doc2vec_models\\Sample_1.d2v.docvecs.* with mmap=None\n",
      "2019-04-07 14:22:31,283 : INFO : loaded D:\\workspace\\Text-Classification-Using-Neural-Networks\\src\\DocumentClassifier\\doc2vec_models\\Sample_1.d2v\n"
     ]
    }
   ],
   "source": [
    "#train_d2v() \n",
    "model.load(reader.get_models_path(), model_name)\n",
    "load_trainset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test dataset\n",
      "Current topic: BUSINESS\n",
      "Current topic: SCIENCE\n",
      "Current topic: SPORTS\n",
      "Current topic: TECH\n",
      "Current topic: TRAVEL\n",
      "Finished loading test set\n"
     ]
    }
   ],
   "source": [
    "load_testset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get accuracy of Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 0.9478038211542249\n",
      "Training complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\john rey juele\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "classifier = NN()\n",
    "print(train_docs)\n",
    "classifier.train(np.array(train_docs, ndmin=2), np.array(train_topics, ndmin=2))\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_guesses = classifier.predict(np.array(test_docs, ndmin=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test_topics, ndmin=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7453073242546927"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(np.array(test_topics, ndmin=2),test_guesses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting unseen document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\workspace\\\\FYP_dataset\\\\testing\\\\unseen_doc.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-506b538de86e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0munseen_document\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_new_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"unseen_doc.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0munseen_document\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munseen_document\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-64e72e4cee24>\u001b[0m in \u001b[0;36mprocess_new_doc\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprocess_new_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mcurr_doc_write\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__testing_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurr_doc_write\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m                     \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimple_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\workspace\\\\FYP_dataset\\\\testing\\\\unseen_doc.txt'"
     ]
    }
   ],
   "source": [
    "unseen_document = reader.process_new_doc(\"unseen_doc.txt\")\n",
    "unseen_document = model.infer_doc(unseen_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = classifier.predict_probability(np.array(unseen_document, ndmin=2))\n",
    "results = results.flatten()\n",
    "ts = [\"BUSINESS\", \"POLITICS\", \"SCIENCE\", \"SPORTS\", \"TECH\", \"TRAVEL\"]\n",
    "for i, t in enumerate(ts):\n",
    "    print(\"{}: {}\".format(t, results[i] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
